{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Đường dẫn đến file HDF5\n",
    "# hdf5_file_path = \"model.hdf5\"\n",
    "\n",
    "# # Đọc file HDF5\n",
    "# with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "#     # Lấy danh sách các khóa (keys) trong file\n",
    "#     keys = list(hdf5_file.keys())\n",
    "    \n",
    "#     # In thông tin về các khóa\n",
    "#     print(f\"Keys in HDF5 file: {keys}\")\n",
    "\n",
    "#     # Đọc dữ liệu từ các nhóm (groups) và tập tin (datasets)\n",
    "#     for key in keys:\n",
    "#         # Kiểm tra nếu key là một nhóm\n",
    "#         if isinstance(hdf5_file[key], h5py.Group):\n",
    "#             print(f\"Group: {key}\")\n",
    "#             # Đọc dữ liệu từ nhóm (group) tại đây nếu cần\n",
    "#         elif isinstance(hdf5_file[key], h5py.Dataset):\n",
    "#             print(f\"Dataset: {key}\")\n",
    "#             # Đọc dữ liệu từ tập tin (dataset) tại đây nếu cần\n",
    "#             data = hdf5_file[key][()]\n",
    "#             print(f\"Data: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Tải mô hình từ file HDF5\n",
    "# with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "#     # Load các thông tin cần thiết từ file HDF5 (tuỳ thuộc vào cấu trúc của file)\n",
    "#     # Ví dụ: input_shape, num_classes, load_weights...\n",
    "\n",
    "#     # Hàm dự đoán\n",
    "#     def predict(data):\n",
    "#         # Thực hiện các bước dự đoán với mô hình đã tải\n",
    "#         # Trả về kết quả dự đoán\n",
    "\n",
    "#     # Streamlit App\n",
    "#     st.title(\"Streamlit HDF5 Model Deployment\")\n",
    "#     user_input = st.text_input(\"Nhập dữ liệu cho dự đoán:\")\n",
    "#     if st.button(\"Dự đoán\"):\n",
    "#         # Chuyển đổi dữ liệu người dùng nhập vào định dạng phù hợp\n",
    "#         user_data = np.random.rand(10, *input_shape)  # Thay thế bằng dữ liệu thực tế từ người dùng\n",
    "#         # Thực hiện dự đoán\n",
    "#         prediction = predict(user_data)\n",
    "#         st.write(f\"Kết quả dự đoán: {prediction}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset name and shape, type\n",
    "# <HDF5 dataset \"default\": shape (1000,), type \"<f8\">\n",
    "# train_filename = 'model.hdf5'\n",
    "# with h5py.File(train_filename, \"r\") as f:\n",
    "#     for file_key in f.keys():\n",
    "#         group = f[file_key]\n",
    "#         print(group)\n",
    "#         try:\n",
    "#             for group_key in group.keys():\n",
    "#                 group2 = group[group_key]\n",
    "#                 print(f\"---->{group2}\")\n",
    "#                 for group_key2 in group2.keys():\n",
    "#                         print(f\"--------->{group2[group_key2]}\")\n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "\n",
    "# train\n",
    "train_filename = 'model.hdf5'\n",
    "datasets = []\n",
    "with h5py.File(train_filename, \"r\") as f:\n",
    "    for file_key in f.keys():\n",
    "        group = f[file_key]\n",
    "        if isinstance(group, h5py._hl.dataset.Dataset):\n",
    "            datasets.append(np.array(group))\n",
    "            continue\n",
    "        for group_key in group.keys():\n",
    "            group2 = group[group_key]\n",
    "            if isinstance(group2, h5py._hl.dataset.Dataset):\n",
    "                datasets.append(np.array(group2))\n",
    "                continue\n",
    "            for group_key2 in group2.keys():\n",
    "                group3 = group2[group_key2]\n",
    "                if isinstance(group3, h5py._hl.dataset.Dataset):\n",
    "                    datasets.append(np.array(group3))\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the root of the file: ['model_weights', 'optimizer_weights']\n",
      "Groups in the root of the file: ['model_weights', 'optimizer_weights']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = 'model.hdf5'  # Replace with the actual file path\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # List all items (groups and datasets) in the root of the file\n",
    "    items = list(file.keys())\n",
    "    print(\"Items in the root of the file:\", items)\n",
    "\n",
    "    # Identify groups by checking their type\n",
    "    groups = [item for item in items if isinstance(file[item], h5py.Group)]\n",
    "    print(\"Groups in the root of the file:\", groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the group 'model_weights': ['activation', 'activation_1', 'activation_2', 'activation_3', 'batch_normalization', 'batch_normalization_1', 'batch_normalization_2', 'batch_normalization_3', 'concatenate', 'concatenate_1', 'conv2d_10', 'conv2d_11', 'conv2d_12', 'conv2d_3', 'conv2d_4', 'conv2d_5', 'conv2d_6', 'conv2d_7', 'conv2d_8', 'conv2d_9', 'dense_3', 'dense_5', 'dropout', 'flatten_1', 'input_2', 'max_pooling2d_10', 'max_pooling2d_11', 'max_pooling2d_4', 'max_pooling2d_5', 'max_pooling2d_6', 'max_pooling2d_7', 'max_pooling2d_8', 'max_pooling2d_9', 'top_level_model_weights']\n",
      "Subgroups in the group 'model_weights': ['activation', 'activation_1', 'activation_2', 'activation_3', 'batch_normalization', 'batch_normalization_1', 'batch_normalization_2', 'batch_normalization_3', 'concatenate', 'concatenate_1', 'conv2d_10', 'conv2d_11', 'conv2d_12', 'conv2d_3', 'conv2d_4', 'conv2d_5', 'conv2d_6', 'conv2d_7', 'conv2d_8', 'conv2d_9', 'dense_3', 'dense_5', 'dropout', 'flatten_1', 'input_2', 'max_pooling2d_10', 'max_pooling2d_11', 'max_pooling2d_4', 'max_pooling2d_5', 'max_pooling2d_6', 'max_pooling2d_7', 'max_pooling2d_8', 'max_pooling2d_9', 'top_level_model_weights']\n",
      "Datasets in the group 'model_weights': []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    group_name = 'model_weights'  # Replace with the actual group name\n",
    "    group = file[group_name]\n",
    "\n",
    "    # List all items (groups and datasets) in the specified group\n",
    "    group_items = list(group.keys())\n",
    "    print(f\"Items in the group '{group_name}':\", group_items)\n",
    "\n",
    "    # Identify groups within the specified group\n",
    "    subgroups = [item for item in group_items if isinstance(group[item], h5py.Group)]\n",
    "    print(f\"Subgroups in the group '{group_name}':\", subgroups)\n",
    "\n",
    "    dataset_names = [name for name in group.keys() if isinstance(group[name], h5py.Dataset)]\n",
    "    print(f\"Datasets in the group '{group_name}':\", dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    group_name = 'optimizer_weights'  # Replace with the actual group name\n",
    "    group = file[group_name]\n",
    "\n",
    "    # List all items (groups and datasets) in the specified group\n",
    "    group_items = list(group.keys())\n",
    "    print(f\"Items in the group '{group_name}':\", group_items)\n",
    "\n",
    "    # Identify groups within the specified group\n",
    "    subgroups = [item for item in group_items if isinstance(group[item], h5py.Group)]\n",
    "    print(f\"Subgroups in the group '{group_name}':\", subgroups)\n",
    "\n",
    "    dataset_names = [name for name in group.keys() if isinstance(group[name], h5py.Dataset)]\n",
    "    print(f\"Datasets in the group '{group_name}':\", dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: ()\n",
      "Dataset value: 8526\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_path, 'r') as file:\n",
    "    dataset = file['optimizer_weights/iteration:0']\n",
    "    data_value  = dataset[()]  # Specify the start and end indices\n",
    "    print(\"Value of the scalar dataset:\", data_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "model_path = 'model.hdf5'  \n",
    "# Load the Keras model from the HDF5 file\n",
    "def load_keras_model(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            model_json = file.attrs['model_config']\n",
    "            model = model_from_json(model_json)\n",
    "            model.load_weights(file_path)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "loaded_model = load_keras_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_methods = [method for method in dir(loaded_model) if callable(getattr(loaded_model, method))]\n",
    "\n",
    "# Print the list of methods\n",
    "print(\"List of methods:\")\n",
    "for method in model_methods:\n",
    "    print(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predictions: [[6.2741542e-06 9.7548115e-01 1.6102545e-08 1.3940635e-07 2.4446208e-02\n",
      "  1.3538022e-06 5.2435149e-05 5.3463797e-08 1.1696996e-05 6.5759872e-07]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img_path = 'luanuoc1.jpg'\n",
    "img = image.load_img(img_path, target_size=(256, 256))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize the image\n",
    "\n",
    "# Make a prediction\n",
    "predictions = loaded_model.predict(img_array)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

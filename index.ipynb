{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Đường dẫn đến file HDF5\n",
    "# hdf5_file_path = \"model.hdf5\"\n",
    "\n",
    "# # Đọc file HDF5\n",
    "# with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "#     # Lấy danh sách các khóa (keys) trong file\n",
    "#     keys = list(hdf5_file.keys())\n",
    "    \n",
    "#     # In thông tin về các khóa\n",
    "#     print(f\"Keys in HDF5 file: {keys}\")\n",
    "\n",
    "#     # Đọc dữ liệu từ các nhóm (groups) và tập tin (datasets)\n",
    "#     for key in keys:\n",
    "#         # Kiểm tra nếu key là một nhóm\n",
    "#         if isinstance(hdf5_file[key], h5py.Group):\n",
    "#             print(f\"Group: {key}\")\n",
    "#             # Đọc dữ liệu từ nhóm (group) tại đây nếu cần\n",
    "#         elif isinstance(hdf5_file[key], h5py.Dataset):\n",
    "#             print(f\"Dataset: {key}\")\n",
    "#             # Đọc dữ liệu từ tập tin (dataset) tại đây nếu cần\n",
    "#             data = hdf5_file[key][()]\n",
    "#             print(f\"Data: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Tải mô hình từ file HDF5\n",
    "# with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "#     # Load các thông tin cần thiết từ file HDF5 (tuỳ thuộc vào cấu trúc của file)\n",
    "#     # Ví dụ: input_shape, num_classes, load_weights...\n",
    "\n",
    "#     # Hàm dự đoán\n",
    "#     def predict(data):\n",
    "#         # Thực hiện các bước dự đoán với mô hình đã tải\n",
    "#         # Trả về kết quả dự đoán\n",
    "\n",
    "#     # Streamlit App\n",
    "#     st.title(\"Streamlit HDF5 Model Deployment\")\n",
    "#     user_input = st.text_input(\"Nhập dữ liệu cho dự đoán:\")\n",
    "#     if st.button(\"Dự đoán\"):\n",
    "#         # Chuyển đổi dữ liệu người dùng nhập vào định dạng phù hợp\n",
    "#         user_data = np.random.rand(10, *input_shape)  # Thay thế bằng dữ liệu thực tế từ người dùng\n",
    "#         # Thực hiện dự đoán\n",
    "#         prediction = predict(user_data)\n",
    "#         st.write(f\"Kết quả dự đoán: {prediction}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset name and shape, type\n",
    "# <HDF5 dataset \"default\": shape (1000,), type \"<f8\">\n",
    "# train_filename = 'model.hdf5'\n",
    "# with h5py.File(train_filename, \"r\") as f:\n",
    "#     for file_key in f.keys():\n",
    "#         group = f[file_key]\n",
    "#         print(group)\n",
    "#         try:\n",
    "#             for group_key in group.keys():\n",
    "#                 group2 = group[group_key]\n",
    "#                 print(f\"---->{group2}\")\n",
    "#                 for group_key2 in group2.keys():\n",
    "#                         print(f\"--------->{group2[group_key2]}\")\n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "\n",
    "# train\n",
    "train_filename = 'model.hdf5'\n",
    "datasets = []\n",
    "with h5py.File(train_filename, \"r\") as f:\n",
    "    for file_key in f.keys():\n",
    "        group = f[file_key]\n",
    "        if isinstance(group, h5py._hl.dataset.Dataset):\n",
    "            datasets.append(np.array(group))\n",
    "            continue\n",
    "        for group_key in group.keys():\n",
    "            group2 = group[group_key]\n",
    "            if isinstance(group2, h5py._hl.dataset.Dataset):\n",
    "                datasets.append(np.array(group2))\n",
    "                continue\n",
    "            for group_key2 in group2.keys():\n",
    "                group3 = group2[group_key2]\n",
    "                if isinstance(group3, h5py._hl.dataset.Dataset):\n",
    "                    datasets.append(np.array(group3))\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "model_weights\n",
      "optimizer_weights\n",
      "_MutableMapping__marker\n",
      "__abstractmethods__\n",
      "__bool__\n",
      "__class__\n",
      "__class_getitem__\n",
      "__contains__\n",
      "__delattr__\n",
      "__delitem__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__getitem__\n",
      "__getnewargs__\n",
      "__getstate__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__iter__\n",
      "__le__\n",
      "__len__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__nonzero__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__reversed__\n",
      "__setattr__\n",
      "__setitem__\n",
      "__sizeof__\n",
      "__slots__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_abc_impl\n",
      "_d\n",
      "_e\n",
      "_gcpl_crt_order\n",
      "_id\n",
      "_ipython_key_completions_\n",
      "_lapl\n",
      "_lcpl\n",
      "attrs\n",
      "build_virtual_dataset\n",
      "clear\n",
      "copy\n",
      "create_dataset\n",
      "create_dataset_like\n",
      "create_group\n",
      "create_virtual_dataset\n",
      "file\n",
      "get\n",
      "id\n",
      "items\n",
      "keys\n",
      "move\n",
      "name\n",
      "parent\n",
      "pop\n",
      "popitem\n",
      "ref\n",
      "regionref\n",
      "require_dataset\n",
      "require_group\n",
      "setdefault\n",
      "update\n",
      "values\n",
      "visit\n",
      "visititems\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('model.hdf5', 'r') as file:\n",
    "    dataset_keys = list(file.keys())\n",
    "\n",
    "# Print the dataset keys\n",
    "print(\"Available datasets:\")\n",
    "for key in dataset_keys:\n",
    "    print(key)\n",
    "\n",
    "with h5py.File('model.hdf5', \"r\") as file:\n",
    "    model = file.get(\"model_weights\") \n",
    "\n",
    "for method in dir(model):\n",
    "    print(method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
